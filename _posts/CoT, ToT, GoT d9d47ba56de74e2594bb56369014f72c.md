---
title: Coming into LLM :CoT, ToT, GoT
date: 2023-09-19 22:11:20
index_img: /cover/3.jpg
author : NA-Wen
category: NLP
tags: [NLP,LLM]
catagories: NLP
---
# CoT, ToT, GoT

éšç€LLM å‚æ•°é‡çš„æ‰©å¢ï¼Œå…¶æ¨ç†èƒ½åŠ›éšä¹‹å¢å¼ºã€‚ä¹Ÿå› æ­¤è¡ç”Ÿå‡ºäº†ä¸€ç³»åˆ—çš„å¸®åŠ© LLM æ›´å¥½çš„å®Œæˆreasoning çš„promptå·¥ç¨‹æŠ€å·§ã€‚
<!--more-->
## quick start of benchmarks

(1)  GSM8K benchmark : math word problems

(2)  SVAMP dataset : math word problems with varying structures 

(3)  ASDiv dataset :diverse math word problems

(4)  AQuA dataset : algebraic word problems

(5)  MAWPS benchmark 

# CoT

## intro

rationale-augmented training and finetuning method : cost a lot 

few-shot prompting  : works poorly on tasks that require reasoning ability , and do not help improve as the size of LLM scales up

Chain of thought : a series of intermediate NL reasoning steps 

<input , chain of thought , output>

## method

Considering oneâ€™s own thought process when solving a complicated problem , it will decompose the problem into many intermediate steps and solve each before giving a final answer . 

![Untitled](/img2/Untitled.png)

explanation :

1. Decomposing the problem into many intermediate steps , meaning that it will cost some computing resource when doing each steps , which costs additional computation more than standard prompting. 
2. A chain of thought provides an interpretable window into the behavior of the model , showing its reasoning path.

Application:

1. arithmetic reasoning (as shown above )
2. commonsense reasoning 
    
    ![Untitled](/img2/Untitled%201.png)
    
3. symbolic reasoning 
    
    ![Untitled](/img2/Untitled%202.png)
    
    ![Untitled](/img2/Untitled%203.png)
    

## discussion

åœ¨æ¨ç†é—®é¢˜ä¸Šï¼Œé€šè¿‡few-shotçš„è®¾ç½®ï¼Œåœ¨promptä¸­é¢å¤–æ‰“å…¥ä¸€äº›QAï¼Œé‡Œé¢ä½¿ç”¨chain of thoughtï¼Œä»¥æ¿€å‘LLM çš„æ¨ç†èƒ½åŠ›ã€‚åœ¨æ•°å­¦æ¨ç†ï¼Œå¸¸è¯†æ¨ç†ï¼Œç¬¦å·æ¨ç†ä¸Šå‡å–å¾—äº†è¾ƒå¥½çš„æ•ˆæœã€‚

ä½†æ˜¯promptéœ€è¦äººä¸ºé¢å¤–å»è®¾ç½®ï¼Œéœ€è¦æ‰‹å†™å¯¹åº”domainä¸­çš„chain of thoughtã€‚åœ¨æ­¤åŸºç¡€ä¹‹ä¸Šï¼Œå°å²›äº”å‘ç°äº†å’’è¯­ ï¼š **Let's think step by step** ï¼Œ ä¸€ä¸¾ä½¿å¾—LLM åœ¨zero-shotçš„æƒ…å†µä¸‹ ï¼Œèƒ½å¤Ÿè‡ªåŠ¨çš„ç”ŸæˆCoTã€‚

åœ¨æ­¤ä¹‹ä¸Šï¼ŒåŒå¹´æ²ç¥çš„å·¥ä½œï¼Œä½¿ç”¨LLM è‡ªè¡Œå»ç”Ÿæˆæ ·ä¾‹ï¼Œä½œä¸ºæç¤ºä¸­çš„few-shotï¼Œç»“æœä¹Ÿå¾ˆå¥½ã€‚

<aside>
ğŸ’¡ å¾ˆé‡è¦çš„äº‹æƒ…æ˜¯ï¼ŒéªŒè¯äº†ä¸€ä¸ªç¬¦åˆå¸¸è¯†çš„insight ï¼šé€æ­¥æ€è€ƒæ˜¯æœ‰åŠ©äºæ¨ç†èƒ½åŠ›çš„æå‡çš„
ä»¥åŠï¼Œå¯ä»¥é€šè¿‡æŸäº› **â€å’’è¯­â€œ**ï¼Œä½¿å¾—LLM å±•ç°å‡ºæ›´å¼ºçš„èƒ½åŠ›ï¼Œç”Ÿæˆå‡ºæ›´åŠ åˆæ„çš„å›ç­”ã€‚

</aside>

ä½†æ˜¯æœ‰ä¸€ä¸ªå°é—®é¢˜æ˜¯ï¼Œ chainä¸­çš„æ¯ä¸€ä¸ªnodeçš„åˆ’åˆ†å¹¶ä¸æ¸…æ¥šï¼Œè¿˜æ˜¯ä»¥ä¸€ä¸ªsequenceçš„å½¢å¼ç»™å‡ºäº†æ•´ä¸ªchainã€‚

# ToT

## intro

In generating , there is a original autoregressive mechanism , making token-level decisions in a left-to-right manner. 

â€Dual  processâ€œ model : people have two modes , a fast ,unconscious mode , a slow , deliberate and conscious mode . In reinforcement learning , when humans or animals explore new circumstances ,they use the two modes , in associative â€˜model freeâ€™ learning and â€˜model basedâ€™ planning. 

Now the simple token-level  choices are reminiscent of â€˜System 1â€™, we may need more â€˜System 2â€™,which can maintains and explores diverse alternatives for current choices instead of just picking one , and evaluates the current status and actively looks ahead or backtracks. 

![Untitled](/img2/Untitled%204.png)

three new tasks :

Game of 24, Creative Writing, and Crosswords

## ToT

two short comings when LM solve problems: 

1) Locally, they do not explore different continuations within a thought process â€“ the branches of the tree. 

2) Globally, they do not incorporate any type of planning, lookahead, or backtracking to help evaluate these different options

four questions that ToT need to answer:

1. How to decompose the intermediate process into thought steps; 
    
    Depending on the problem , a thought could be a couple of words (Crosswords), a line of equation (Game of 24), or a whole paragraph of writing plan (Creative Writing).
    
    but should be small enough , that LLM can generate several in one step
    

 2.  How to generate potential thoughts from each state; 

sample : generates thoughts for some times .this works better when the thought space is large 

propose : use a propose prompt , generate all thoughts in one sequence to avoid duplication , this works better when the thought space is constraint 

 3. How to heuristically evaluate states; 

State evaluator should evaluate the progress ,serving as a heuristics for the search algorithm to determine which states to keep exploring and in which order. Here use LLM to deliberately reason about the state. 

Value  

Vote 

 4. What search algorithm to use.

BFS : only remain several best results in one step

DFS : just choose the best result to go forward

## discussion

æ¯”è¾ƒé‡è¦çš„å› ç´ åœ¨äºåŒæ—¶ç”Ÿæˆå¤šä¸ªæ ·æœ¬ï¼Œè¿›è¡Œè¯„ä¼°åæœ‰é€‰æ‹©åœ°å‘ä¸‹æœç´¢ã€‚ç›¸æ¯”äºCoTæ¥è¯´ï¼Œæ ·æœ¬ç©ºé—´å¤§äº†å¾ˆå¤šï¼Œå’ŒCoT-SCæ¯”èµ·æ¥ä¹ŸåŒç†ï¼Œæœç´¢ç©ºé—´å¤§äº†å¾ˆå¤šã€‚

ä½†æ˜¯æ— è®ºæ˜¯evaluatorçš„è¯„ä¼°æ‰“åˆ†/vote ï¼Œè¿˜æ˜¯æœç´¢ç®—æ³•ï¼Œå®é™…éƒ½ä¸å¤ªç§‘å­¦ï¼Œéƒ½æœ‰å¾ˆå¤šå€¼å¾—argueçš„åœ°æ–¹ï¼ˆæ¯”æ–¹è¯´DFS BFSæœç´¢åªçœ‹å½“å‰æ­¥éª¤çš„è¯„åˆ†çœŸçš„å¯ä¿¡å—ï¼Œ LLM æ‰“åˆ†å‚è€ƒçš„æ˜¯ä»€ä¹ˆï¼Œè¿˜æœ‰å¯¹äºchainçš„æ‹†åˆ†å¤ªè¿‡äºæš´åŠ›ï¼‰ 

æ‰€ä»¥è¿˜æ˜¯è®¤ä¸ºæ˜¯å› ä¸ºæœç´¢ç©ºé—´å¤§äº†ï¼Œç®—åŠ›æ¶ˆè€—å¤šäº†ï¼Œå¯¼è‡´ç»“æœå¥½ã€‚å¦‚æœç®—ä¸€ä¸ªpass@t ä¼°è®¡ä¸ä¸€å®šæœ‰CoTå¼ºã€‚ä¹‹åå¦‚æœæœ‰æ›´åˆç†/æ›´å¥½çš„æœç´¢ç®—æ³•ï¼Œå¯èƒ½ä¼šä»æ¶ˆè€—å’Œæ•ˆèƒ½çš„trade-offä¸Šéƒ½æœ‰ç€æå‡ã€‚

# GoT

## intro

Model the thought generation process of LLM into arbitrary graph, vertices represent thoughts , and edges correspond to the dependencies of vertices.

GoT:  well-suited for tasks that can be naturally decomposed into smaller subtasks that are solved individually and then merged for a final solution.

volume of a thought: for a given thought v, the volume of thought is the the number of thoughts that can directly connected to the original thought through edge.

## GoT

![Untitled](/img2/Untitled%205.png)

a tuple :

$$
(G,\tau,\epsilon, r)
$$

$G:$ the process of LLM thoughts

$\tau:$ the potential thought transformation

$\epsilon :$  the evaluator function to obtain thoughts scores

$r:$ the ranking function to select the most relevant thoughts

### reasoning process

A vertex contains a solution at hand , (be it an initial , an intermediate , a final )

The concrete form of a thought depends on the concrete problem. Sometimes graph nodes belong to different classes, then we add a heterogeneous graph  $G=(V,E,c)$ ,where c represents the mapping of node to its classes.

### transformations of thoughts

each transformation can be seen as $\tau(G,p_\theta)$ , and $G^{'}=\tau(G,p_\theta)$ .  $\tau$ modifies G usually by adding new vertices and their incoming edges, then generates a new graph.

- aggregation transformation
    
    create one vertex by aggregating some thoughts 
    
- refining transformation
    
    refine the current vertex ,and add a loop on the vertex (but no new edge)
    
- generation transformations
    
    Based on an existing thought , generate one or more new thoughts.
    

### scoring and ranking thoughts

$\epsilon(v,G,p_\theta)$ : score thought v ,by considering the thought process G 

$R(G,p_\theta,h)$: choose the top h thoughts which scores are the highest h. And the specific form of R depends on a use case.

## discussion

åœ¨ç”Ÿæˆå¤šä¸ªæ ·æœ¬ä»¥è¾¾åˆ°æ¢ç´¢æ›´å¤§çš„å›ç­”ç©ºé—´çš„åŸºç¡€ä¸Šï¼Œå¢åŠ äº†æ›´å¤šçš„å˜æ¢æ“ä½œï¼Œä»¥å®ç°æ›´çµæ´»çš„æ•ˆæœã€‚åŒæ—¶æå‡ºäº†volumeï¼Œ è€ƒè™‘äº†æ¶ˆè€—å’Œæ•ˆèƒ½ä¹‹é—´çš„trade-off ï¼Œæ˜¯ä¸€ä¸ªæ›´åˆç†çš„è¯„ä¼°thought chain ä¼˜ç§€ç¨‹åº¦çš„æ–¹å¼ã€‚

é—®é¢˜è¿˜æ˜¯åœ¨äºï¼Œæ‰“åˆ†çš„æœºåˆ¶ä½¿å¾—åªèƒ½åŸºäºLLMå»åšå†³å®šï¼Œä¸è¿‡åœ¨è¯„åˆ†ä¸­è€ƒè™‘å…¨ä½“çš„thought processï¼ˆèµ·ç å…¬å¼ä¸­çœ‹èµ·æ¥æ˜¯çš„ï¼‰è¿˜æ˜¯ç›¸å¯¹æ¥è¯´æ¯”è¾ƒåˆç†ï¼Œé¿å¼€äº†ä¸ºäº†å½“å‰æœ€ä¼˜è€Œå¿½ç•¥å…¨å±€çš„é™·é˜±ã€‚åŒæ—¶ï¼Œgraphæ˜¯è¦è‡ªå·±é€šè¿‡å…·ä½“é—®é¢˜è‡ªå·±è®¾è®¡çš„ï¼Œæ­¤ç‚¹æ˜¯å¦èƒ½å¤Ÿå°½é‡è‡ªåŠ¨åŒ–è®©LLM å»å®Œæˆï¼Ÿæ¯”å¦‚è¯´ç”¨ç®€å•çš„multi generation+ sample+nodeåˆ¤æ–­â€¦â€¦