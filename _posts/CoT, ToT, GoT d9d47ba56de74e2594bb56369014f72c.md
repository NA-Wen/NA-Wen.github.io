---
title: Coming into LLM :CoT, ToT, GoT
date: 2023-09-19 22:11:20
index_img: /cover/3.jpg
author : NA-Wen
category: NLP
tags: [NLP,LLM]
catagories: NLP
---
# CoT, ToT, GoT

随着LLM 参数量的扩增，其推理能力随之增强。也因此衍生出了一系列的帮助 LLM 更好的完成reasoning 的prompt工程技巧。
<!--more-->
## quick start of benchmarks

(1)  GSM8K benchmark : math word problems

(2)  SVAMP dataset : math word problems with varying structures 

(3)  ASDiv dataset :diverse math word problems

(4)  AQuA dataset : algebraic word problems

(5)  MAWPS benchmark 

# CoT

## intro

rationale-augmented training and finetuning method : cost a lot 

few-shot prompting  : works poorly on tasks that require reasoning ability , and do not help improve as the size of LLM scales up

Chain of thought : a series of intermediate NL reasoning steps 

<input , chain of thought , output>

## method

Considering one’s own thought process when solving a complicated problem , it will decompose the problem into many intermediate steps and solve each before giving a final answer . 

![Untitled](/img2/Untitled.png)

explanation :

1. Decomposing the problem into many intermediate steps , meaning that it will cost some computing resource when doing each steps , which costs additional computation more than standard prompting. 
2. A chain of thought provides an interpretable window into the behavior of the model , showing its reasoning path.

Application:

1. arithmetic reasoning (as shown above )
2. commonsense reasoning 
    
    ![Untitled](/img2/Untitled%201.png)
    
3. symbolic reasoning 
    
    ![Untitled](/img2/Untitled%202.png)
    
    ![Untitled](/img2/Untitled%203.png)
    

## discussion

在推理问题上，通过few-shot的设置，在prompt中额外打入一些QA，里面使用chain of thought，以激发LLM 的推理能力。在数学推理，常识推理，符号推理上均取得了较好的效果。

但是prompt需要人为额外去设置，需要手写对应domain中的chain of thought。在此基础之上，小岛五发现了咒语 ： **Let's think step by step** ， 一举使得LLM 在zero-shot的情况下 ，能够自动的生成CoT。

在此之上，同年沐神的工作，使用LLM 自行去生成样例，作为提示中的few-shot，结果也很好。

<aside>
💡 很重要的事情是，验证了一个符合常识的insight ：逐步思考是有助于推理能力的提升的
以及，可以通过某些 **”咒语“**，使得LLM 展现出更强的能力，生成出更加合意的回答。

</aside>

但是有一个小问题是， chain中的每一个node的划分并不清楚，还是以一个sequence的形式给出了整个chain。

# ToT

## intro

In generating , there is a original autoregressive mechanism , making token-level decisions in a left-to-right manner. 

”Dual  process“ model : people have two modes , a fast ,unconscious mode , a slow , deliberate and conscious mode . In reinforcement learning , when humans or animals explore new circumstances ,they use the two modes , in associative ‘model free’ learning and ‘model based’ planning. 

Now the simple token-level  choices are reminiscent of ‘System 1’, we may need more ‘System 2’,which can maintains and explores diverse alternatives for current choices instead of just picking one , and evaluates the current status and actively looks ahead or backtracks. 

![Untitled](/img2/Untitled%204.png)

three new tasks :

Game of 24, Creative Writing, and Crosswords

## ToT

two short comings when LM solve problems: 

1) Locally, they do not explore different continuations within a thought process – the branches of the tree. 

2) Globally, they do not incorporate any type of planning, lookahead, or backtracking to help evaluate these different options

four questions that ToT need to answer:

1. How to decompose the intermediate process into thought steps; 
    
    Depending on the problem , a thought could be a couple of words (Crosswords), a line of equation (Game of 24), or a whole paragraph of writing plan (Creative Writing).
    
    but should be small enough , that LLM can generate several in one step
    

 2.  How to generate potential thoughts from each state; 

sample : generates thoughts for some times .this works better when the thought space is large 

propose : use a propose prompt , generate all thoughts in one sequence to avoid duplication , this works better when the thought space is constraint 

 3. How to heuristically evaluate states; 

State evaluator should evaluate the progress ,serving as a heuristics for the search algorithm to determine which states to keep exploring and in which order. Here use LLM to deliberately reason about the state. 

Value  

Vote 

 4. What search algorithm to use.

BFS : only remain several best results in one step

DFS : just choose the best result to go forward

## discussion

比较重要的因素在于同时生成多个样本，进行评估后有选择地向下搜索。相比于CoT来说，样本空间大了很多，和CoT-SC比起来也同理，搜索空间大了很多。

但是无论是evaluator的评估打分/vote ，还是搜索算法，实际都不太科学，都有很多值得argue的地方（比方说DFS BFS搜索只看当前步骤的评分真的可信吗， LLM 打分参考的是什么，还有对于chain的拆分太过于暴力） 

所以还是认为是因为搜索空间大了，算力消耗多了，导致结果好。如果算一个pass@t 估计不一定有CoT强。之后如果有更合理/更好的搜索算法，可能会从消耗和效能的trade-off上都有着提升。

# GoT

## intro

Model the thought generation process of LLM into arbitrary graph, vertices represent thoughts , and edges correspond to the dependencies of vertices.

GoT:  well-suited for tasks that can be naturally decomposed into smaller subtasks that are solved individually and then merged for a final solution.

volume of a thought: for a given thought v, the volume of thought is the the number of thoughts that can directly connected to the original thought through edge.

## GoT

![Untitled](/img2/Untitled%205.png)

a tuple :

$$
(G,\tau,\epsilon, r)
$$

$G:$ the process of LLM thoughts

$\tau:$ the potential thought transformation

$\epsilon :$  the evaluator function to obtain thoughts scores

$r:$ the ranking function to select the most relevant thoughts

### reasoning process

A vertex contains a solution at hand , (be it an initial , an intermediate , a final )

The concrete form of a thought depends on the concrete problem. Sometimes graph nodes belong to different classes, then we add a heterogeneous graph  $G=(V,E,c)$ ,where c represents the mapping of node to its classes.

### transformations of thoughts

each transformation can be seen as $\tau(G,p_\theta)$ , and $G^{'}=\tau(G,p_\theta)$ .  $\tau$ modifies G usually by adding new vertices and their incoming edges, then generates a new graph.

- aggregation transformation
    
    create one vertex by aggregating some thoughts 
    
- refining transformation
    
    refine the current vertex ,and add a loop on the vertex (but no new edge)
    
- generation transformations
    
    Based on an existing thought , generate one or more new thoughts.
    

### scoring and ranking thoughts

$\epsilon(v,G,p_\theta)$ : score thought v ,by considering the thought process G 

$R(G,p_\theta,h)$: choose the top h thoughts which scores are the highest h. And the specific form of R depends on a use case.

## discussion

在生成多个样本以达到探索更大的回答空间的基础上，增加了更多的变换操作，以实现更灵活的效果。同时提出了volume， 考虑了消耗和效能之间的trade-off ，是一个更合理的评估thought chain 优秀程度的方式。

问题还是在于，打分的机制使得只能基于LLM去做决定，不过在评分中考虑全体的thought process（起码公式中看起来是的）还是相对来说比较合理，避开了为了当前最优而忽略全局的陷阱。同时，graph是要自己通过具体问题自己设计的，此点是否能够尽量自动化让LLM 去完成？比如说用简单的multi generation+ sample+node判断……