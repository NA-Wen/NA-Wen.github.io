---
title: Basic knowledge of machine learning (3) 
date: 2023-09-25 23:31:11
index_img: /img/linearpost.png
tags: [nomalization,MLE,math]
category: MLBasic 
author : NA-Wen
---
å›ç­”äº†æœºå™¨å­¦ä¹ è¿‡ç¨‹ä¸­ä¸‰ä¸ªåŸºæœ¬é—®é¢˜(1. what is the hypothesis function? 2. what is the loss function? 3. how do we solve the training problem?). åŒ…å«äº†éƒ¨åˆ†æ•°å­¦è¯æ˜ã€‚
<!--more-->
***This note is compiled based on the course materials provided by Professor Mingsheng Long in course "machine learning" at tsinghua university.*** 

every machine learning algorithm should solve three basic problem :

1. what is the hypothesis function?
2. what is the loss function?
3. how do we solve the training problem?

# An example : predict the peak power consumption in summer

- first do **feature engineering** : which attribute is x ,which is y
- why called â€œregressionâ€? label space   $y=R$
- EDA (exploratory data analysis ) to ease model selection
    - done before you choose model
    - not a function ? more x attributes needed(éœ€è¦æ›´å¤šçš„è‡ªå˜é‡/feature)
- linear regression
    - feature vector : $x\in R^{d}$, label : $y\in R$
    - whole hypothesis space **h**
        
        ![Untitled](/img3/Untitled.png)
        
        (w corresponds to each high-dim plane in hypothesis space)
        
    - loss
        - use the squared loss(L2 loss)
            
            ![Untitled](/img3/Untitled%201.png)
            
    - process
        
        ![Untitled](/img3/Untitled%202.png)
        

Until here , we have solved the first two problems , then comes to a new problem : how we solve the training problem? 

## optimization

- how to compute w matrix?
    - Analytic solution?
        
        $$
        data \;matrix:\;X=\begin{bmatrix}--x_1^{T}-- \\ --x_2^{T}-- \\ --x_3^{T}-- \\ . \\ . \\ . \\ --x_n^{T}--
        \end{bmatrix}
        $$
        
        $$
        label\;matrix:\;y=\begin{bmatrix}y_1 \\ y_2 \\ . \\ . \\ . \\ y_n\end{bmatrix}
        $$
        
        $$
        \hat{\epsilon}=\sum_{i=1}^{n}(w^Tx_i-y_i)^2 \\ =||Xw-y||^2 \\ =(Xw-y)^{T}(Xw-y) \\ =w^{T}X^{T}Xw-w^{T}Xy-y^{T}Xw+y^{T}y
        $$
        
        $$
        \frac{\partial\hat{\epsilon}}{\partial w}=\frac{\partial w^{T}X^{T}Xw}{\partial w}-\frac{\partial w^{T}X^{T}y}{\partial w}-\frac{\partial y^{T}Xw}{\partial w}+0 \\ =2X^{T}Xw-2X^{T}y
        $$
        
        $$
        w=(X^{T}X)^{-1}X^{T}y
        $$
        
        the complexity is unaffordable : $O(d^2(d+n))$ , and always more dim in features when using linear model 
        
    - so instead , we do optimization !
- optimization
    - (for differentiable loss function) GD (gradient descent): follow the steepest slope , *but you only observe local information for each step*
        - gradient :
            
            $$
            g=\nabla J(w)  \\ g_j=\nabla_jJ(w)
            $$
            
            According to taylor approximation :
            
            $$
            J(w)=J(w_0)+(w-w_0)^Tg+...
            $$
            
            so we can get : 
            
            $$
            J(w)=J(w_0-\eta g)-\eta g^Tg
            $$
            
            different order taylor approximation â†’ different order optimization method (one-order :faster ; two-order : more accurate )
            
            è¿™é‡Œç”¨çš„æ˜¯ä¸€é˜¶æ³°å‹’å±•å¼€å¯¹åº”çš„ä¼˜åŒ–æ–¹æ³•
            
        - iteration
            - do until converge : T
                
                $$
                w^{t+1}\leftarrow w^t-\eta(2X^{T}Xw-2X^{T}y) 
                $$
                
            - learning rate $\eta$  matters
                
                ![Untitled](/img3/Untitled%203.png)
                
            - complexity $O(dnT)$
            - Theoretically, convergence rate is $\frac{1}{\sqrt T}$ under convex condition.(can be proved)
    - SGD (stochastic gradient descent)
        - randomly sample a small batch (size of m)
            
            $$
            w^{t+1}\leftarrow w^t-\eta(2X_m^{T}X_mw-2X_m^{T}y) 
            $$
            
        - complexity $O(dmT)$

So we know how to complement the training process , but why do we use L2 loss in the process? Now give a proof / explanation from statistical view.

## statistical : MLE

First, supposed we have a parametric model $\{p(z;\theta)|\theta\in \Theta\}$ 

- **( assumption 1 )** $D=\{z_1,z_2,...,z_n\}$ç‹¬ç«‹åŒåˆ†å¸ƒ

$$
p(D;\hat{\theta})=\prod_{i=1}^np(z_i;\hat{\theta})
$$

due to numerical stability , we prefer to work with log-likelihood 

$$
log (p(D;\hat{\theta}))=\sum_{i=1}^nlogp(z_i;\hat{\theta})
$$

Maximum Likelihood Estimator (MLE) for estimating the parameter
 $\theta$ in the parametric model$\{p(z;\theta)|\theta\in \Theta\}$

$$
\hat{\theta}\in argmax_{\theta\in \Theta}\sum_{i=1}^nlogp(z_i;\hat{\theta})
$$

The MLE leads to a particular loss function.

We all know in supervised learning , one $z$ contains $x,y$(feature ,label) ; so the p can be

$$
\prod_{i=1}^np(z_i;\hat{\theta})=\prod_{i=1}^np(y_i|x_i;\hat{\theta})p(x_i)
$$

Getting $p(x_i)$ is difficult (in fact for generation model we get this), so we do another assumption here.


- **( assumption 2 )** å¯¹æ ·æœ¬çš„åˆ†å¸ƒè®¤ä¸ºæ˜¯ç»éªŒåˆ†å¸ƒï¼Œä½†æ˜¯è¿™ç§å‡è®¾ç¼ºä¹å¯¹äºæ ·æœ¬å½¢çŠ¶çš„å…ˆéªŒå‡è®¾ï¼Œå› æ­¤å¯¹äºå¯¹æŠ—æ ·æœ¬é²æ£’æ€§å˜å¼±ã€‚


<aside>
ğŸ’¡ "ç»éªŒåˆ†å¸ƒ" æ˜¯ä¸€ä¸ªç»Ÿè®¡å­¦å’Œæ•°æ®åˆ†æé¢†åŸŸçš„æœ¯è¯­ï¼Œé€šå¸¸æŒ‡çš„æ˜¯æ ¹æ®å·²ç»è§‚æµ‹åˆ°çš„æ•°æ®æ ·æœ¬æ¥ä¼°è®¡ä¸€ä¸ªéšæœºå˜é‡çš„æ¦‚ç‡åˆ†å¸ƒã€‚å®ƒæ˜¯ä¸€ç§éå‚æ•°ä¼°è®¡æ–¹æ³•ï¼Œä¸ä¾èµ–äºå¯¹åˆ†å¸ƒå½¢çŠ¶çš„å…ˆéªŒå‡è®¾ã€‚
ç»éªŒåˆ†å¸ƒé€šå¸¸é€šè¿‡å°†æ•°æ®æ ·æœ¬ä¸­çš„æ¯ä¸ªè§‚æµ‹å€¼è§†ä¸ºä¸€ä¸ªæ¦‚ç‡è´¨ç‚¹æ¥è¡¨ç¤ºã€‚è¿™æ„å‘³ç€æ¯ä¸ªè§‚æµ‹å€¼éƒ½æœ‰ä¸€ä¸ªä¸ä¹‹ç›¸å…³çš„æ¦‚ç‡è´¨ç‚¹ï¼Œæ¦‚ç‡è´¨ç‚¹çš„é«˜åº¦ï¼ˆæˆ–æƒé‡ï¼‰ç­‰äºè¯¥è§‚æµ‹å€¼åœ¨æ ·æœ¬ä¸­å‡ºç°çš„é¢‘ç‡ã€‚

åœ¨æ•°å­¦ç¬¦å·ä¸­ï¼Œå‡è®¾æœ‰ä¸€ä¸ªåŒ…å«è§‚æµ‹å€¼çš„æ•°æ®æ ·æœ¬é›†åˆ $x_1, x_2, \ldots, x_n$ï¼Œé‚£ä¹ˆè¿™äº›è§‚æµ‹å€¼çš„ç»éªŒåˆ†å¸ƒå¯ä»¥è¡¨ç¤ºä¸ºï¼š

$$
\hat{F}(x) = \frac{1}{n} \sum_{i=1}^{n} I(x_i \leq x)
$$

</aside>

$$
\prod_{i=1}^np(z_i;\hat{\theta})=\prod_{i=1}^np(y_i|x_i;\hat{\theta})p(x_i)=\prod_{i=1}^np(y_i|x_i;\hat{\theta})\frac{1}{n}
$$

We can only focus on the  conditional probability.

$$
\hat{\theta}=max_{\theta\in \Theta}\sum_{i=1}^np(y_i|x_i;\hat{\theta})
$$

- **( assumption 3 )** add gaussian noise assumption: å› ä¸ºè§‚æµ‹ä¸­å¯èƒ½æœ‰å™ªéŸ³ï¼Œå‡è®¾å™ªéŸ³æœä»é«˜æ–¯åˆ†å¸ƒ
    
    $$
    y=w^Tx+e \\ e\sim\N(0,\sigma^2)
    $$
    

Adding the noise does not change the expectation. 

![Untitled](/img3/Untitled%204.png)

Then the y should also belong to a gaussian distribution

- **( assumption 4 )** linear model

$$
y\sim \N(w^Tx,\sigma^2)
$$

$$
\frac{1}{n}\sum_{i=1}^nlogp(y_i|x_i;\hat{\theta}) \\ =\frac{1}{n}\sum_{i=1}^nlog(\frac{1}{\sqrt{2\pi}\sigma}exp(-\frac{1}{2\sigma^2}(y_i-w^Tx_i)^2)) \\ =\frac{1}{n}(\frac{n}{2}log(\frac{1}{2\pi\sigma^2})-\sum_{i=1}^n\frac{1}{2\sigma^2}(y_i-w^Tx_i)^2))
$$

Until here , we know maximum Likelihood Estimation of w under the Gaussian noise assumption is equivalent to linear regression with the squared loss.

*long-tailed distribution :å¯¹äºå™ªå£°æ›´åŠ é²æ£’ï¼›åœ¨åˆ†å¸ƒä¸­å·²ç»è€ƒè™‘è¿›æ¥äº† â†’ æ­¤ç§æƒ…å†µä¸‹æŸå¤±å‡½æ•°é•¿ä»€ä¹ˆæ ·ï¼Ÿ*

![Untitled](/img3/Untitled%205.png)

![Untitled](/img3/Untitled%206.png)

# Nonlinearization

- feature map  xâ†’z
    - basis function : the mapping function, non linear transform
    - polynomial function
    - **RBF (radial basis function)**
        
        ![Untitled](/img3/Untitled%207.png)
        
    
    å°†åŸæœ¬çš„ç‰¹å¾åšäº†æ˜ å°„ï¼Œè¾¾åˆ°äº†éçº¿æ€§çš„æ•ˆæœã€‚åšå¤šæ¬¡ä¸åŒçš„æ˜ å°„å¯ä»¥è·å¾—ç‰¹å¾ä¸Šçš„å¤šæ ·æ€§ã€‚ä½†æ˜¯åœ¨å›å½’è¿‡ç¨‹ä¸­è¿˜æ˜¯åšçš„çº¿æ€§é¢„æµ‹ã€‚æ•´ä½“ä¸Šæé«˜äº†æ¨¡å‹çš„æ‹Ÿåˆèƒ½åŠ›ï¼Œæ‹¥æœ‰äº†æ‹Ÿåˆéçº¿æ€§å‡½æ•°çš„èƒ½åŠ›ã€‚
    
    ä¾ç„¶éœ€è¦EDAæ¥åšç‰¹å¾çš„é€‰æ‹©ï¼
    
- local weighting
    - å°†å±€éƒ¨éƒ½æ˜¯è§†ä¸ºçº¿æ€§çš„å‡½æ•°ï¼Œç”¨å±€éƒ¨é™„è¿‘çš„ä¿¡æ¯å»é¢„æµ‹å±€éƒ¨
    - å’Œtransformerçš„æ€æƒ³ç±»ä¼¼ ? é€šè¿‡ä½œqueryå’Œkeyçš„åŒ¹é…ï¼ˆç›¸ä¼¼åº¦é«˜ç›¸å½“äºxè·ç¦»å¾ˆè¿‘ï¼‰ï¼Œæ¥å¯¹äºvalueåŠ æƒã€‚è®¤ä¸ºå’Œå½“å‰tokenæ›´ç›¸ä¼¼tokensçš„å¯¹äºå½“å‰tokençš„embeddingæ›´é‡è¦ã€‚

åœ¨å¼•å…¥äº†éçº¿æ€§æ¨¡å‹ä¹‹åï¼Œè™½ç„¶æ¨¡å‹çš„æ‹Ÿåˆèƒ½åŠ›å¢å¼ºäº†ï¼Œä½†æ˜¯ä¹Ÿæ›´å®¹æ˜“å‡ºç°è¿‡æ‹Ÿåˆçš„é—®é¢˜ã€‚æˆ‘ä»¬é€šè¿‡æ­£åˆ™åŒ–ï¼ˆæ­£è§„åŒ–ï¼‰æ¥è§£å†³ã€‚

# regularization

how to deal with overfitting ?

- an observation
    
    when overfitting appear , the weight parameters are too large 
    
    belike :
    
    ![Untitled](/img3/Untitled%208.png)
    
    So constraining the size of parameters is a direct way to escape from overfitting. 
    
- L1 L2 Norm
    - L2 regularized linear regression (ridge regression) :commonly used
        
        ![Untitled](/img3/Untitled%209.png)
        
        equals to :
        
        ![Untitled](/img3/Untitled%2010.png)
        
        <aside>
 
        ğŸ’¡ æ‹‰æ ¼æœ—æ—¥ä¹˜å­æ³•æ˜¯ä¸€ç§ç”¨äºæ±‚è§£å¸¦æœ‰çº¦æŸæ¡ä»¶çš„æå€¼é—®é¢˜çš„æ–¹æ³•ï¼Œé€šå¸¸ç”¨äºä¼˜åŒ–é—®é¢˜ã€‚ä¸‹é¢æ˜¯ä½¿ç”¨æ‹‰æ ¼æœ—æ—¥ä¹˜å­æ³•æ±‚è§£æå€¼çš„ä¸€èˆ¬æ­¥éª¤ï¼š
        å‡è®¾ä½ æœ‰ä¸€ä¸ªå¸¦æœ‰çº¦æŸæ¡ä»¶çš„ä¼˜åŒ–é—®é¢˜ï¼Œè¦æœ€å¤§åŒ–ï¼ˆæˆ–æœ€å°åŒ–ï¼‰å‡½æ•° $f(x, y, \ldots)$
        
        ç›®æ ‡å‡½æ•°ï¼š$f(x, y, \ldots)$
        
        çº¦æŸæ¡ä»¶ï¼š$g(x, y, \ldots)=c$
        
        å…¶ä¸­ $x, y, \ldots$æ˜¯ä½ è¦ä¼˜åŒ–çš„å˜é‡ï¼Œ*c* æ˜¯å¸¸æ•°ã€‚è¦ä½¿ç”¨æ‹‰æ ¼æœ—æ—¥ä¹˜å­æ³•æ±‚è§£è¿™ä¸ªé—®é¢˜ï¼Œä½ å¯ä»¥æŒ‰ç…§ä»¥ä¸‹æ­¥éª¤è¿›è¡Œï¼š
        
        1. **å»ºç«‹æ‹‰æ ¼æœ—æ—¥å‡½æ•°ï¼š** å®šä¹‰ä¸€ä¸ªæ–°çš„å‡½æ•°ï¼Œç§°ä¸ºæ‹‰æ ¼æœ—æ—¥å‡½æ•°ï¼ˆLagrangianï¼‰ï¼Œå®ƒæ˜¯ç›®æ ‡å‡½æ•°å’Œçº¦æŸæ¡ä»¶çš„çº¿æ€§ç»„åˆï¼Œå¸¦æœ‰ä¸€ä¸ªé¢å¤–çš„å‚æ•°ï¼ˆæ‹‰æ ¼æœ—æ—¥ä¹˜å­ï¼‰ï¼š
            
            $$
            L(x, y, \ldots, \lambda) = f(x, y, \ldots) - \lambda \cdot (g(x, y, \ldots) - c)
            $$
            
            è¿™é‡Œï¼Œ*Î»* æ˜¯æ‹‰æ ¼æœ—æ—¥ä¹˜å­ã€‚
            
        2. **è®¡ç®—æ‹‰æ ¼æœ—æ—¥å‡½æ•°çš„åå¯¼æ•°ï¼š** å¯¹æ‹‰æ ¼æœ—æ—¥å‡½æ•° $L(x,y,â€¦,Î»)$ åˆ†åˆ«å¯¹ $x,y,â€¦,Î»$æ±‚åå¯¼æ•°ï¼Œå¹¶ä»¤å®ƒä»¬ç­‰äºé›¶ï¼š
        3. **è§£æ–¹ç¨‹ç»„ï¼š** ä»¥æ‰¾åˆ°æœ€ä¼˜çš„  $x,y,â€¦,Î»$ã€‚è¿™äº›å€¼å°†ç»™å‡ºç›®æ ‡å‡½æ•°åœ¨æ»¡è¶³çº¦æŸæ¡ä»¶ä¸‹çš„æå€¼ã€‚
        </aside>
        
        ![Untitled](/img3/Untitled%2011.png)
        
    
    when $Î»$ is larger , the red part is more ?
    
    - L1 : better for sparse solution
        
        ![Untitled](/img3/Untitled%2012.png)
        
        ![Untitled](/img3/Untitled%2013.png)
        
    
    å‚æ•°çš„ç¨€ç–æ€§æ˜¯è¡¡é‡æ¨¡å‹å¤æ‚åº¦çš„é‡è¦æŒ‡æ ‡ï¼Œæ­£åˆ™åŒ–çš„æ“ä½œ(å›¾åƒä¸Šçœ‹æ˜¯æœ‰normå°–ç‚¹)å¯ä»¥æœ‰æ•ˆåœ°åœ¨æ­¤ç‚¹ä¸Šäºˆä»¥æ”¹å–„ã€‚
    
    - L0
        
        ![Untitled](/img3/Untitled%2014.png)
        
        ![Untitled](/img3/Untitled%2015.png)
        
        ![Untitled](/img3/Untitled%2016.png)
        
    - L1
        
        ![Untitled](/img3/Untitled%2017.png)
        
        ![Untitled](/img3/Untitled%2018.png)
        
        ![Untitled](/img3/Untitled%2019.png)
        
    - L2
        
        ![Untitled](/img3/Untitled%2020.png)
        
        ![Untitled](/img3/Untitled%2021.png)
        
        ![Untitled](/img3/Untitled%2022.png)
        
    
    ä¸‰ç§æ­£åˆ™åŒ–æ“ä½œéƒ½æ˜¯é€šè¿‡é™åˆ¶å‚æ•°çš„å¤æ‚ç¨‹åº¦æ¥é™åˆ¶æ¨¡å‹çš„å¤æ‚ç¨‹åº¦ã€‚ä¸åŒçš„è®¾ç½®å¯¼è‡´å‚æ•°çš„å—é™æƒ…å†µä»¥åŠå¯¼è‡´çš„å‚æ•°å¤æ‚åº¦çš„è¡¡é‡ä¸åŒã€‚ä½†æ˜¯L2å¯¹äºå‚æ•°ç¨€ç–åŒ–çš„æ•ˆæœæ¯”ä¸ä¸Šå‰ä¸¤ä¸ªï¼ˆæ›´å¤šæ˜¯smooth optimizationï¼‰ï¼›è€ŒL0ç›¸æ¯”äºL1åˆæ˜¯éå‡¸çš„ã€‚
    
    Why regularization works? We can explain it through Lipschitz constant (ğŸ¤© really an amazing solution! )
    
    $$
    lipschitz\; theorem:D(f(x_i),f(x_j))\leq C* D(x_i,x_j)
    $$
    
    $$
    D(f(x_i+\Delta),f(x_i)) \\ =D(w(x+\Delta),wx) \\  =||w\Delta||_2^2 
    $$
    
    So C here is $||w||_2^2$
    
    å¯¹äºå…¶ä»–çš„èŒƒæ•°ï¼Œæ­¤ç§è¯æ˜ä¹Ÿæ˜¯åˆé€‚çš„ã€‚éƒ½å¯ä»¥å¾—åˆ°å‚æ•°$w$çš„èŒƒæ•°ä½œä¸ºLipschitzå¸¸æ•°å‡ºç°ã€‚å½“å‡½æ•°è‡ªå˜é‡å˜åŒ–æ¯”è¾ƒå°æ—¶ï¼Œå¦‚æœfå€¼å˜åŒ–ä¹Ÿè¾ƒå°ï¼Œæ­¤æ—¶å‡½æ•°ä¸€èˆ¬å…·æœ‰æ›´å¥½çš„å…‰æ»‘æ€§å’Œæ›´æ…¢çš„å˜åŒ–é€Ÿç‡ï¼Œè¿™æ ·çš„å‡½æ•°ä¸€èˆ¬æ¥è¯´æ˜¯æ›´åˆæ„çš„ã€‚
    
    è¿™å°±è§£é‡Šäº†ä¸ºä»€ä¹ˆæˆ‘ä»¬æœ‰æ ¹æ®èŒƒæ•°å¯¼å‡ºçš„ä¸åŒæ­£åˆ™åŒ–æ–¹æ³•ã€‚