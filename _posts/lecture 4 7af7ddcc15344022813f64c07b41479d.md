---
title: Basic knowledge of machine learning (5) 
date: 2023-10-08 23:41:20
index_img: /img/linearpost.png
tags: [SVM,math,optimization]
category: MLBasic 
author : NA-Wen
---
ä»‹ç»SVM æ”¯æŒå‘é‡æœºçš„æ¥å†ï¼Œä¼˜åŒ–æ–¹æ³•ï¼ŒæŸå¤±å‡½æ•°æ¥å†ã€‚
<!--more-->
***This note is compiled based on the course materials provided by Professor Mingsheng Long in course "machine learning" at tsinghua university.*** 

# lecture 4

# supporting vector machine

## introduction

Unlike we treat classification problem from a regression solution, here we just want to find a linear classifier to do the classification. From a geometry view, we want to find a hyperplane. 

New convention: 

- explicitly write the intercept b
- change {0,1} to {-1,1}

So under below situation, though all hyperplanes can separate the two classes, which one is the best classifier?

 

![Untitled](img5/Untitled.png)

two standards :

1. the largest margin to the training data (robust for noisy data)
2. high confidence classification (far is better)

**Margin:** Twice of the distance to the closest points of either class.
-Twice of the largest noise that can be tolerated by the classifier

![Untitled](img5/Untitled%201.png)

**So how to find the classifier with the largest margin?**

## margin

two requirements :

- the margin is the largest
- classify all data points correctly

It becomes a constrained optimization problem (find the largest margin and constrained by the right classification):

$$
max_{w,b}\;margin(w,b)
$$

$$
s.t.\;y_i(wx_i+b)\geq 1,\;1\leq i\leq n
$$

Here 1 is the margin here, but can be any positive integer.

![Untitled](img5/Untitled%202.png)

> recall: point-hyperplane distance 
point $x_0$, hyperplane with a normal vector $w$ and a interpret b ($wx+b=0$)
> 

$$
distance=\frac{|w(x_0-x)|}{||w||}=\frac{|wx_0+b|}{||w||}
$$

### hard-margin support vector machine

When the points closest to the margin lie on $wx+b=1/-1$, the margin $\gamma$ is  

$$
\gamma=\frac{2}{||w||_2}
$$

![Untitled](img5/Untitled%203.png)

same as(the convention due to$||w||_2$ is convex function but $\frac{1}{||w||_2}$ is non-convex ):

![Untitled](img5/Untitled%204.png)

**This is only for the linearly separable case.
Hardly used in practice!**

### soft-margin support vector machine

- sometimes under linearly non-separable case , no solution
- small margin may cause overfitting

So we add a slack variables  $\xi$ 

Then the problem becomes:

$$
min_{w,b,\xi}\frac{1}{2}||w||_2^2,\; s.t.\;y_i(wx_i+b)\geq1-\xi_i
$$

$$
\xi_i\geq0,\;\sum_{i=1}^{n}\xi_i\leq n^{'},1\leq i\leq n 
$$

(every x has and is constrained by a slack variable , but the sum of all slack variables cannot be too large)

## Soft-SVM for multiclass classification

The original SVM can only solve two-class classification. If we want to do multi classed classification, can we train a set of classifications , each one classify the i-th class from others and then select the largest value ?

Obviously, comparing their prediction values may not make much senseâ€¦ This approach is not as elegant as softmax regression for no probability produced.

# optimization

## constrained optimization

Consider a general optimization problem:

$$
min_x\;f(x),\; s.t. \;x\in X
$$

Finding the minimum is an optimization process, and the â€œbelonging toâ€ demonstrates a constraint.

A convex optimization:

![Untitled](img5/Untitled%205.png)

 Understanding a convex function is easy: any chord of the function is above the function curve/ any tangent of the function is below the function curve.

### equality constraints

$$
min_x\;f(x),\; s.t. \;g(x)=0
$$

![Untitled](img5/Untitled%206.png)

The closure is $g(x)=0$, so the gradient is orthogonal to the tangent of the surface.

And the gradient of f(x) needs to be orthogonal to the tangent. Because the gradient represents the changing direction of the function, the minimum point on the curve can not decrease its value by moving along a small distance along the curve.

That is to say:

$$
\nabla f+\mu \nabla g=0, for \;\mu \neq 0 
$$

Lagrangian function of this problem is defined by:

![Untitled](img5/Untitled%207.png)

- a d+1 dimension equation
- Stationary point x gives the f(x) minimum.

### inequality constraints

$$
min_x\;f(x),\; s.t. \;g(x)\leq0
$$

![Untitled](img5/Untitled%208.png)

If the point lies on the curve, the same as equality constraint. Else, solve what?

The solution to the constrained optimization problem with inequality constraints is yielded by optimizing the **Lagrangian function.**

$$
L(x,\lambda)=f(x)+\lambda g(x)
$$

**Karush-Kuhn-Tucker (KKT) conditions :**

![Untitled](img5/Untitled%209.png)

### general lagrangian function

**function:**

$$
L(x,\lambda, \mu)=f(x)+\sum_{j=1}^{J}\lambda_jg_j(x)+\sum_{k=1}^{K}\mu_kh_k(x)
$$

**KKT conditions:**

$$
primal \;feasibility:\;g_j(x)\leq 0 , h_k(x)=0
$$

$$
dual\;feasibility:\;\lambda_j\geq 0
$$

$$
complementary\;slackness:\;\lambda_jg_j(x)=0
$$

And:

![Untitled](img5/Untitled%2010.png)

Optimal ?

![Untitled](img5/Untitled%2011.png)

One optimal $x$, one $\lambda, \mu$ corresponding.

## dual problem: lagrangian method

The primal problem: 

![Untitled](img5/Untitled%2012.png)

converting into dual problem :

![Untitled](img5/Untitled%2013.png)

æ•´ä¸ªæ‹‰æ ¼æœ—æ—¥å‡½æ•°å¯¹xæ±‚å¯¼çš„è¿‡ç¨‹ï¼Œå®é™…ä¸Šæ˜¯æ±‚Lå‡½æ•°å¯¹äºxçš„æœ€å°å€¼ã€‚ç›¸å¯¹åº”æ­¤æ±‚å¾—çš„æœ€å°å€¼ï¼Œä¼šæœ‰å¯¹åº”çš„$\lambda,\mu$ã€‚å¯¹äºxçš„çº¦æŸè¾ƒä¸ºå¤æ‚ï¼Œæˆ‘ä»¬æ›´å¸Œæœ›å…³æ³¨$\lambda,\mu$ã€‚

å›ºå®šå˜é‡xï¼Œå¯¹äº$\lambda,\mu$æ±‚Lå‡½æ•°çš„æœ€å¤§å€¼ï¼Œå®é™…ä¸Šæ˜¯æ­¤é—®é¢˜çš„å¯¹å¶é—®é¢˜ã€‚

èƒŒåçš„åŸç†æ˜¯ï¼š

å¯¹ä¸€ä¸ªå¤šå˜é‡å‡½æ•°ï¼Œå…ˆæ±‚ï¼ˆå…³äºæŸäº›å‚æ•°çš„ï¼‰æœ€å°å†æ±‚ï¼ˆå…³äºå…¶ä»–å‚æ•°çš„ï¼‰æœ€å¤§ï¼Œå’Œå…ˆæ±‚æœ€å¤§å†æ±‚æœ€å°æ¯”èµ·æ¥ï¼Œå‰è€…å°äºç­‰äºåè€…ã€‚

---

Back to the Soft-SVM problem, 

![Untitled](img5/Untitled%2014.png)

Here we add $\xi$ constraint into the L function. The lagrangian function as follows: 

![Untitled](img5/Untitled%2015.png)

$w,b,\xi_i$ are variables (like x). 

![Untitled](img5/Untitled%2016.png)

Substituting and do the dual problem:

![Untitled](img5/Untitled%2017.png)

Then the KKT conditions:

![Untitled](img5/Untitled%2018.png)

Why called **Supporting vectors ?** 

When $\alpha$ is 0, the point plays no effect. So the vectors playing a role are called supporting vector. 

Thinking back to regularization, we want to get sparse parameters, meaning giving up some less important features.

> I donâ€™t like constrained optimization. I donâ€™t like one-off method.
Give me my SGD back!
> 

## primal problem : SGD

The former soft-SVM can be written into 

![Untitled](img5/Untitled%2019.png)

Same like the following format, which can be directly solved from importing somethingâ€¦

![Untitled](img5/Untitled%2020.png)

In another way :

![Untitled](img5/Untitled%2021.png)

![Untitled](img5/Untitled%2022.png)

The theorem can be  naturally proved. A lot of unnecessary trailsâ€¦.ğŸ˜µ

So what is the loss function? - Hinge Loss function!

![Untitled](img5/Untitled%2023.png)

![Untitled](img5/Untitled%2024.png)

(ä»è¿™é‡Œå¯ä»¥å¾—çŸ¥ï¼Œåœ¨ç›‘ç£å­¦ä¹ ä¸­ï¼Œå¯¹äºæŸå¤±å‡½æ•°çš„é€‰æ‹©å®é™…ä¸Šæ˜¯è¦å’Œobjective functionç›¸ç»Ÿä¸€ï¼Œæ‰èƒ½æ–½åŠ åˆç†çš„æƒ©ç½šï¼›ä¸è‡³äºå¯¼è‡´æŸäº›dataåœ¨æŸå¤±å‡½æ•°ä¸­èµ·åˆ°ä¸å¿…è¦çš„å½±å“/å½±å“è¿‡å¤§ã€‚æˆ–è€…è¯´å¯ä»¥çœ‹ä½œé€‰æ‹©æŸç§å‘é‡èŒƒæ•°çš„é—®é¢˜ï¼Œåˆé€‚çš„å‘é‡èŒƒæ•°è¦æ±‚èƒ½å¤Ÿåœ¨å½“ä¸‹çš„é—®é¢˜åœºæ™¯ä¸­æ°å½“çš„è¡¡é‡ä¸¤ä¸ªå‘é‡ä¹‹é—´çš„è·ç¦»ã€‚è™½ç„¶æœ‰é™ç»´çº¿æ€§ç©ºé—´ä¸­æ‰€æœ‰çš„å‘é‡èŒƒæ•°æ˜¯ç­‰ä»·çš„ï¼Œä½†è¿™ä»…ä»…æ˜¯ä»å¤§å°çš„è§’åº¦ï¼Œå¹¶ä¸è¶³ä»¥è¡¡é‡å®Œæ•´çš„èŒƒæ•°ä¿¡æ¯)

Then just do SGD:

![Untitled](img5/Untitled%2025.png)

![Untitled](img5/Untitled%2026.png)